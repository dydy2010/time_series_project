---
editor: 
  markdown: 
    wrap: 72
---

# Analysis of SNB policy rates and Swiss inflation rates

## Data evaluation & preparation

### libraries

```{r}
library(tidyverse)
library(lubridate)
library(readxl)
library(zoo)
library(tseries)
library(forecast)
library(lmtest)
library(stats)
library(quantmod)
library(vars)
library(car)
```

### Load data

```{r}
policy_rate_data <- read_excel("../data/snb-data-snbgwdzid-en-all-20250414_1000.xlsx",
                               col_types = c("text", "numeric", "numeric", "numeric", "numeric", "numeric", "numeric", "numeric"),
                               skip = 21)

inflation_data <- read_excel("../data/snb-data-plkoprinfla-en-all-20250422_0900.xlsx",
                             skip = 14) #skipping the first 14 rows

```

### First trial analysis using Dataset 1

Our first dataset (Dataset 1) for the SNB policy rates provides several
columns that could be used as **SNB policy rate**. But they are all
**only available for specific time windows**, in particular the column
labeled 'SNB policy rate' is only **available since June 2019**. So we
have to decide which (or which combination) would be best for our
analysis.

```{r}
pr_full <- policy_rate_data %>% 
  as_tibble() %>% 
  dplyr::select(date = "Overview",
         policy = "SNB policy rate",
         ir_above = "Interest rate on sight deposits above threshold",
         sar_fix = "SARON fixing at the close of the trading day",
         special = "Special rate  (Liquidity-shortage financing facility)") %>%
  mutate(date = ymd(date),
         (across(c(policy, ir_above, sar_fix, special),
                 ~ round(as.numeric(.), 2)))
  )

ggplot(pr_full, aes(x = date)) +
  geom_line(aes(y = policy, color = "Policy Rate")) +
  geom_line(aes(y = ir_above, color = "Interest rate above threshold")) +
  geom_line(aes(y = sar_fix, color = "SARON fixing")) +
  geom_line(aes(y = special, color = "Special rate")) +
  scale_color_manual(values = c("Policy Rate" = "blue",
                                "Interest rate above threshold" = "green",
                                "SARON fixing" = "brown",
                                "Special rate" = "red")) +
  labs(title = "Swiss Policy Rates: Available data",
       color = "Legend",
       y = "(Policy-Rates") +
  theme_minimal() +
  theme(legend.position = "bottom", legend.direction = "horizontal") 
```

### Average of 3 months Libor upper/lower limits as proxy for missing SNB policy rate data before 2020.

After we have learned that the SNB's policy rate before 2020 was
actually a range, namely the **upper and lower limits** of the 3 months
Libor, we found another dataset (Dataset 2) with this data. Visual
inspection shows that the average of the upper and lower limits indeed
fits perfectly well to the policy rates afterwards.

```{r}
libor_data <- suppressWarnings(
  read_excel("../data/snb-target rate-policy rate-2000-2025.xlsx",
                         range = cell_limits(c(18, 1), c(NA, 4)),
                         col_names = c("date", "policy_rate", "libor_3m_low", "libor_3m_high"),
                         col_types = c("text", "numeric", "numeric", "numeric")) %>%
  mutate(date = ymd(str_c(date, "-01")),
         (across(c(policy_rate, libor_3m_low, libor_3m_high),
                 ~ round(as.numeric(.), 2))),
         libor_3m_avg = (libor_3m_low + libor_3m_high) / 2) %>% 
  mutate(libor_3m_avg = round(libor_3m_avg, 2))
)

ggplot(libor_data, aes(x = date)) +
  geom_line(aes(y = policy_rate, color = "Policy Rate")) +
  geom_line(aes(y = libor_3m_low, color = "libor_3m_lower")) +
  geom_line(aes(y = libor_3m_high, color = "libor_3m_upper")) +
  geom_line(aes(y = libor_3m_avg, color = "libor_3m_avg")) +
  scale_color_manual(values = c("Policy Rate" = "blue",
                                "libor_3m_lower" = "green",
                                "libor_3m_upper" = "brown",
                                "libor_3m_avg" = "lightblue")) +
  labs(title = "Swiss Policy Rates, 3 Month Libor lower and upper limits",
       color = "Legend",
       y = "(Policy-Rates") +
  theme_minimal() +
  theme(legend.position = "bottom", legend.direction = "horizontal") 

```

### Further data preparation, data merge and timeseries object

We calculated the **average of the limits** of the 3 months Libor and
used it as policy rate until June 2019 and switch to the official SNB
policy rate after June 2019.

```{r}
pr <- libor_data %>%
  mutate(
  policy_rate = if_else(
    date < as.Date("2019-06-01"),
    libor_3m_avg,       
    policy_rate)) %>% 
    dplyr::select(date, policy_rate)

infl <- inflation_data %>% 
  as_tibble() %>%
  dplyr::select(date = Overview, infl = `SNB - Core inflation, trimmed mean`) %>% 
  mutate(date = ymd(str_c(date, "-01")),   # add a '-01' to the date string before making it a date
         infl = as.numeric(infl),
         infl = round(infl, 1))


# merge data

df <- inner_join(pr, infl, by = "date")  # merge the two tibbles


# convert df to a zoo time series object

df_ts <- zoo(
  df %>% dplyr::select(-date),
  order.by = df$date
)

```

### Final look at the data we use for analysis

A look at the data we use for our time series analysis suggests that
there is a **relation** between SNB **policy rates** and Swiss
**inflation rates**, although it is not obvious which one is triggering
the other. Furthermore it shows very different behavior regarding
fluctuation: **Inflation rates change monthly** (actually daily),
whereas **policy rates** are **sometimes constant** for longer time
periods.

```{r}
ggplot(df, aes(x = date)) +
  geom_line(aes(y = policy_rate, color = "SNB Policy Rate")) +
  geom_line(aes(y = infl, color = "SNB - Core inflation, trimmed mean")) +
  scale_color_manual(values = c("SNB Policy Rate" = "blue",
                                "SNB - Core inflation, trimmed mean" = "red")) +
  labs(title = "Swiss Policy Rates and Inflation Rates 2000-2025",
       color = "Legend",
       y = "(Inflation- / Policy-) Rates") +
  theme_minimal() +
  theme(legend.position = "bottom", legend.direction = "horizontal")

```

## Stationarity & linear regression models

### Stationarity

Initially both data series are **not stationary** (p-values \> 0.05).
**After calculating the differences** from one month to the next, the
Augmented Dickey-Fuller Test shows that **both series are stationary**
now with p-values = 0.01 each.

```{r}
adf.test(na.omit(df_ts$policy_rate))
adf.test(na.omit(df_ts$infl))

df_differenced <- diff(df_ts)
adf.test(na.omit(df_differenced$policy_rate))
adf.test(na.omit(df_differenced$infl))
```

### Correlations

The p-value is 0.06. It shows a very **week positive correlation**
between policy rates and inflation rates.

```{r}
cor(df_differenced$policy_rate, df_differenced$infl, use = "pairwise.complete.obs")
```

### Basic Linear regression model

A linear regression model of policy_rate and inflation rates shows **no
significant coefficients** and **R squared is very low**.

```{r}
lin_reg <- lm(infl ~ policy_rate, data = df_differenced)
summary(lin_reg)
```

### Residual analysis (as an exercise)

#### Visual inspection

**Range of Residuals**

-   Most residuals are between **-0.2 and 0.2** → This suggests a
    **moderate prediction error**. There is information left that is not
    used in the current model.
-   **2 outliers** (±0.5) are present → These are **not necessarily
    problematic** unless they’re influential (we check later for Cook's
    distance).

**Wavelike Pattern**

-   The residual plot shows a **wave or sinusoidal pattern**, that
    suggests **non-linearity** or **autocorrelation** in the data.
-   In a good linear model, residuals should be **randomly scattered**
    around zero (no pattern).

```{r}
resid <- lin_reg$residuals
plot(y=resid, x=as.Date(time(df_differenced)), ylab="Residuals", xlab="Year", type="l", main="Regression Residuals")
grid()
```

#### Breusch-Pagan test

-   **Test** for **heteroskedasticit**y (i.e., **changing variance** of
    residuals).
-   Null hypothesis: Residuals have constant variance.
-   Interpretation: If p \> 0.05, fails to reject the null → residuals
    are homoscedastic → Good, the **residuals are homoscedastic.**
-   p\<0.05 rejects the null hypothesis → residuals would be
    heteroskedastic

```{r}
bptest(lin_reg)
```

#### Shapiro test

-   Test for **residual**'s normality
-   Null hypothesis that residuals are **normally distributed**.
-   Test shows a strong rejection of the null hypothesis: The residuals
    of the model are **not normally** distributed.
-   With n = 302 we might disregard non-normality.

```{r}
shapiro.test(resid)
```

#### Outliers and influencial points

There are two large values of Cook's distance on **2008-11-01**,
**2023-03-01 and 2001-09-01**. The first with Cook's distance \>0.6 is
**highly influential**, the other with Cook's distance around 0.3 is
**moderately influential**. High Cook’s distance values indicate impact
on the model's coefficient, but it requires **further inspections**
(e.g. is it an outlier or a data error), then a decision on how this
data point should be treated (transformed or removed) should be made.

After checking the dates in the data, we did find major policy rate
changes:

-   2001_09_01 was a full 1% step down from 2.75% to 1.25%, starting a
    continuous decrease to 0%.
-   2008_11-01 was the last 0.5% step of a continuous decrease of the
    policy rate from 2.25 to 0.
-   2023-03-01 the policy rate was the last 0.5% step in a continuous
    interest rate hike from -0.75%.

These 3 events are reflection of substantial economic interventions by
the SNB, so we tend towards **accepting these residuals as facts**.

```{r}
# plot(lin_reg, which = 1)  # Residuals vs Fitted
# plot(lin_reg, which = 2)  # Q-Q plot
plot(lin_reg, which = 4)  # Cook's distance
```

#### Durbin-Watson test

-   Test for serial correlation
-   Null hypothesis that residuals are not autocorrelated
-   Test statistic is very close to 2, which is the expected value under
    the null hypothesis of no autocorrelation (p \> 0.05). There is no
    statistically significant evidence of positive autocorrelation in
    the residuals.

```{r}
dwtest(lin_reg) 
```

#### Summary and Interpretation of our residual tests:

-   **Breusch-pagan** test for Heteroskedasticity: showed residual with
    constant variance (**homoskedasticity**)
-   **Shapiro** test: **rejects** the null hypothesis of **normally**
    distributed residuals, however with sample size n=302, the
    non-normality may **not have great impact**.
-   **Cook's distance**: several events that had great influence on the
    coefficient are **in fact** **substantial events**, so they are not
    errors.
-   **Durbin Watson** test: **no significant autocorrelation** in
    residuals.

All residual tests prove that our linear model is valid and has no data
quality issues.

So **why** the small R-squared? It doesn't necessarily mean that the
model is wrong, the **reasons** could be that:

-   Inflation is affected by **many more factors**.
-   Time series data in economic events and shocks can be **noisy.**
-   Linear relationship is **weak but statistically significant** and
    economically justifiable.

```{=latex}
\newpage
```

### Alternatives to the basic linear model

#### Alternative 1: Lead-lag relation: infl(t) = a + b \* policy_rate(t-1) + e(t)

Create lagged variables and fit the linear model, removing rows with NA
due to lagging.

```{r}
df_differenced$policy_rate_lag1 <- stats::lag(df_differenced$policy_rate, k = 1)
df_differenced$policy_rate_lag2 <- stats::lag(df_differenced$policy_rate, k = 2)
df_differenced$policy_rate_lag3 <- stats::lag(df_differenced$policy_rate, k = 3)
df_differenced$policy_rate_lag4 <- stats::lag(df_differenced$policy_rate, k = 4)
df_differenced$policy_rate_lag5 <- stats::lag(df_differenced$policy_rate, k = 5)
df_differenced$policy_rate_lag6 <- stats::lag(df_differenced$policy_rate, k = 6)
df_differenced$policy_rate_lag7 <- stats::lag(df_differenced$policy_rate, k = 7)
df_differenced$policy_rate_lag8 <- stats::lag(df_differenced$policy_rate, k = 8)
df_differenced$policy_rate_lag9 <- stats::lag(df_differenced$policy_rate, k = 9)
df_differenced$policy_rate_lag10 <- stats::lag(df_differenced$policy_rate, k = 10)
df_differenced$policy_rate_lag11 <- stats::lag(df_differenced$policy_rate, k = 11)
df_differenced$policy_rate_lag12 <- stats::lag(df_differenced$policy_rate, k = 12)

lin_reg_lagged <- lm(infl ~ policy_rate_lag1 + policy_rate_lag2 + policy_rate_lag3 +
                       policy_rate_lag4 + policy_rate_lag5 + policy_rate_lag6 +
                       policy_rate_lag7 + policy_rate_lag8 + policy_rate_lag9 +
                       policy_rate_lag10 + policy_rate_lag11 + policy_rate_lag12,
                     data = na.omit(df_differenced))
summary(lin_reg_lagged)
```

**Interpretation:**

Lag 1 and lag 10 are significant (p-value \< 0.05). This could imply
**some short- and delayed reaction** of inflation to past policy
decisions. But R squared (0.06249, adjusted 0.02187) is very low, **only
6% of the variance (fluctuations)** in inflation can be explained by the
model.

Lag 1 could make sense regarding the time frame and the model shows that
it is statistically significant. But the direction of lag 1 is not as
expected: a higher policy rate goes with higher inflation.

Lag_10 could make sense as the estimate is negative. Though it is
**common** that a change in the policy rate has a **quite delayed
effect** on inflation rates, but we wonder **why only Lag_10** shows
this effect and nearby months don't? The overall credibility of the lag
effect is **questionable**.

**Our conclusion**: As only 6% of variation is explained by the model
and the two significant lags are against theory or questionable, we are
**not convinced** by this model.

#### Alternative 2: Treat SNB actions as events

```{r}
df_differenced$event_2008_10 <- ifelse(index(df_differenced) >= as.Date("2008-10-01"), 1, 0)
df_differenced$event_2014_11 <- ifelse(index(df_differenced) >= as.Date("2014-11-01"), 1, 0)
df_differenced$event_2020_01 <- ifelse(index(df_differenced) >= as.Date("2020-07-01"), 1, 0)
df_differenced$event_2022_05 <- ifelse(index(df_differenced) >= as.Date("2022-05-01"), 1, 0)
df_differenced$event_2022_10 <- ifelse(index(df_differenced) >= as.Date("2022-10-01"), 1, 0)

lin_reg_events <- lm(infl ~ event_2008_10 + event_2014_11 + event_2020_01 + event_2022_05 + event_2022_10, data = na.omit(df_differenced))
summary(lin_reg_events)
```

**Our conclusion**: The last event 2022_10 is significant. This was a
month after the SNB had increased their policy rate from negative
(-0.25) to positive (+0.50). This is plausible. But again R squared is
very low.

```{=latex}
\newpage
```

## Excursus:

Closer look at inflation only (auto/direct correlations and an ARIMA
model)

### Correlations

**Autocorrelations**

The series show **weak to moderate positive autocorrelation** at lags 4
and 6, and a negative autocorrelation at lag 12. The inflation changes
(infl) today are somewhat positively related to those 4, and 6 months
ago. But inflation 12 months ago tends to move in the opposite direction
from today’s.

```{r}
acf(coredata(na.omit(df_differenced$infl)), lag.max = 12)
```

**Direct correlations**

Direct correlation between a time series and lag k, controlling for all
shorter lags (1 to k−1).

```{r}
pacf(coredata(na.omit(df_differenced$infl)), lag.max = 12)
```

**Rule of thumb regarding ARIMA parameters q and p**

-   Use acf() to choose the q in MA(q) models. -\> q = 4 or 6

-   Use pacf() to choose the p in AR(p) models. -\> p = 4 or 6

The ACF and PACF plots suggest that the series may be modeled as an
ARMA(4,4) or AR(6,6).

## Akaike information criterion: AIC

Identifying the orders p and q of the ARIMA(p,1,q)-model by testing
different model specifications. We only allow a maximum of six AR- and
MA-terms and set the order of integration d to 1.

```{r}
max.order <- 6
d <- 1
```

Defining the matrix in which the values of the AICs for different model
specifications are stored. Then calculating and storing the AICs for
different model specifications.

```{r}
arima_aic <- matrix(NA, ncol=max.order+1, nrow=max.order+1)
row.names(arima_aic) <- c(0:max.order) # Order of AR(p) in rows
colnames(arima_aic) <- c(0:max.order) # Order of MA(q) in columns

for(i in 0:max.order){
  for(j in 0:max.order){
    arima_aic[i+1,j+1]<-Arima(y=df_differenced$infl, order=c(i,d,j), include.constant =  FALSE)$aic
  }
}
arima_aic
index <- which(arima_aic == min(arima_aic), arr.ind = TRUE)
ar <- as.numeric(rownames(arima_aic)[index[1]])
ma <- as.numeric(colnames(arima_aic)[index[2]])
c(ar, ma)
arima_aic[ar+1, ma+1]
```

**Interpretation**: The optimal ARMA-model is ARMA(6,0) with an AIC of
-398.5921. (d according to order of integration).

## ARIMA model

We convert data to a ts object from zoo and estimate the optimal
ARIMA-model (incl. testing for significance of the coefficients)

```{r}
infl_diff_ts <- ts(coredata(df_differenced$infl), frequency = 12)
arima <- Arima(y=infl_diff_ts, order=c(ar,d,ma), include.constant = FALSE)
print(arima)
coeftest(arima)
```

**Interpretation**:

**ARIMA(6,1,0)**: This means the model includes: 6 autoregressive (AR)
terms, 1 difference (change in inflation), 0 moving average (MA) terms.

All AR coefficients (ar1 to ar6) are **negative**, meaning **reverting
behavior** (past increases in inflation are followed by decreases).

**ar1 to ar5** are **significant** (p-values almost 0), indicating
strong predictive power. Inflation show **strong autoregressive
behavior** and **tendency to revert.**

```{r}
arima_5_1_0 <- Arima(infl_diff_ts, order=c(5,1,0))
print(arima_5_1_0)
coeftest(arima_5_1_0)
```

The **ARIMA(5,1,0)** is superior with one coefficient less. So we
proceed with this.

As this is just an Excursus, not much helping to identify the relations
between policy rates and inflation, we skip here the evaluation of
residuals. But - as another exercise - we do a forecasting of inflation
based on this ARIMA model.

### Forecast the next 12 months of inflation

```{r}
forecast_arima <- forecast(arima_5_1_0, h = 12)
print(forecast_arima)

forecast_arima$mean        # Point forecasts
forecast_arima$lower       # Lower bounds (80% and 95%)
forecast_arima$upper       # Upper bounds (80% and 95%)

autoplot(forecast_arima) + 
  ggtitle("ARIMA Forecast for Inflation") +
  xlab("Time") + ylab("Inflation Change")
```

**Last known and forecasted inflation levels**

We get the last date from indexed too object, generate 12 monthly
forecast dates and produce forecast table and plot.

```{r}
last_infl <- tail(na.omit(df$infl), 1)
forecast_changes <- forecast_arima$mean
forecast_inflation <- cumsum(forecast_changes) + last_infl
forecast_upper <- cumsum(forecast_arima$upper[,2]) + last_infl
forecast_lower <- cumsum(forecast_arima$lower[,2]) + last_infl

last_date <- tail(index(df_differenced), 1)
forecast_dates <- seq(from = as.Date(last_date) %m+% months(1), by = "month", length.out = 12)

forecast_table <- data.frame(
  Date = forecast_dates,
  Forecast_Inflation = round(as.numeric(forecast_inflation), 3),
  Forecast_Change = round(as.numeric(forecast_arima$mean), 3),
  Lower_95 = round(forecast_arima$lower[,2], 3),
  Upper_95 = round(forecast_arima$upper[,2], 3)
)

print(forecast_table)

ggplot(forecast_table, aes(x = Date, y = Forecast_Inflation)) +
  geom_line(color = "blue", linewidth = 1) +
  geom_point(color = "blue") +
  labs(
    title = "Forecasted Inflation Levels",
    x = "Date",
    y = "Inflation Rate (%)"
  ) +
  theme_minimal()
```

After this extensive **Excursus** we now go **back to our main topic**,
explaining interactions between SNB policy rates and inflation rates in
Switzerland.

## Vector autoregression and Granger causality

### Do policy rates explain inflation rates?

```{r}
VAR_model <- VAR(cbind(df_differenced$policy_rate, df_differenced$infl) , ic="AIC", lag.max = 12)
# coeftest(VAR_model)
# summary(VAR_model)
causality(VAR_model, cause="df_differenced.policy_rate")["Granger"]
```

The Granger Causality Test (VAR) examines whether past policy rates help
**predict current** values of **inflation** beyond what's already
explained by past values of inflation.

There is **statistically significant evidence that past policy rates
Granger-cause inflation**, i.e., policy rates **have predictive power**
for inflation in our model. **But remember:** Granger causality is not
proof of true causation, it **only indicates predictive** helpfulness.

### Do inflation rates explain policy rates?

**No**: Inflation rates do not Granger-cause the policy rates.

```{r}
causality(VAR_model, cause="df_differenced.infl")["Granger"]
```

### Do major SNB changes in policy rates ('events') explain inflation rates.

There is **strong evidence that event_2022_10 Granger-causes inflation**
(at lag 5 and lag6). This means, that the change in the **SNB policy
rate in autumn 2022 influenced** the Swiss inflation rates.

```{r}
VAR_df <- na.omit(cbind(df_differenced$event_2022_10, df_differenced$infl))
colnames(VAR_df) <- c("event_2022_10", "infl")
VAR_model <- VAR(VAR_df , ic="AIC", lag.max = 12)
coeftest(VAR_model)
causality(VAR_model, cause="event_2022_10")["Granger"]
```

**Residual analysis** We now do a residual analysis based on the last
VAR model with Event_2022_10 to check the quality of the model.

```{r}
Resid_VAR <- resid(VAR_model)
p <- VAR_model$p
residual_dates <- tail(index(df_differenced), -p) # Remove first p dates
plot(x = residual_dates,
     y = Resid_VAR[,1],  # First equation's residuals
     type = "l",
     ylab = "VAR residuals diff inflation",
     xlab = "Year")

# Plotting ACF, histogram, and Q-Q-plot of residuals
acf(data.frame(Resid_VAR), main="VAR residuals diff inflation") # ACF of residuals
hist(Resid_VAR, breaks=25, main="Histogram of residuals", xlab="VAR residuals diff log inflation") # Histogram of residuals

# For a single equation's residuals (e.g., first variable)
residuals_to_plot <- Resid_VAR[,1]  # Select first column

# 1. Q-Q plot with grid
qqnorm(residuals_to_plot, main = "Q-Q Plot of VAR Residuals")
qqline(residuals_to_plot, col = "red")
grid()

# Residual tests
arch.test(VAR_model) # ARCH-LM test for constant variance, null hypothesis = Residuals are homoscedastic
normality.test(VAR_model) # Jarque-Bera test for normality, null hypothesis = Residuals are normally distributed
serial.test(VAR_model) # Portmanteau test (default) for serial correlation, null hypothesis = Residuals are not autocorrelated

```

Summary of residual tests:

The **ARCH-LM** test: for constant variance with the null hypothesis
that residuals are homoscedastic shows with a p-value of 0.1685, that
**residuals are homoscedastic**.

The **Jarque-Bera** test: for normality with the null hypothesis that
residuals are normally distributed shows with a p-value close to zero,
that **residuals are not normally distributed**.

The Portmanteau test: for serial correlation with the null hypothesis
that residuals are not auto-correlated shows with a p-value of 0.8502,
that the **residuals are not auto-correlated**.

The peak in residuals around the **October 2022** event: may reflect the
**impact of a major policy intervention**, aligning with the significant
Granger causality result. After checking the dates, 2022 October was
exactly where SNB was continuously **lowering rates** for months, due to
economic slow down in Europe, global **energy crisis**, and a **25% draw
back** of global **stock markets**.

**Overall interpretation of residual analysis**:

Residuals are not **heteroskedastic or auto_correlated,** but the
non-normality remains an issue. It **might** be neglected as we have 302
observations (large n tends towards normality). The model is overall
valid, though incorporating other factors or considering nonlinear
models in futures studies could be beneficial.
